{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCCsn5FOGeCgabbtyA3MZn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitachawdhary/Poem_Segmentation-/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCYMjcqsgwjW",
        "outputId": "ef4ba4b9-9dd8-4640-d5cd-38b4503a08fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.7/224.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sentence-transformers-3.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install transformers sentence-transformers nltk numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Preprocessing the Poem\n",
        "def preprocess(poem):\n",
        "    sentences = sent_tokenize(poem)\n",
        "    return sentences\n",
        "\n",
        "# Semantic Analysis\n",
        "def emotion(sentences):\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    embeddings = model.encode(sentences)\n",
        "\n",
        "    emotion_pipeline = pipeline('sentiment-analysis', model='j-hartmann/emotion-english-distilroberta-base')\n",
        "    emotions = [emotion_pipeline(sentence)[0] for sentence in sentences]\n",
        "\n",
        "    return embeddings, emotions\n",
        "\n",
        "# Identify Semantic Shifts\n",
        "\n",
        "def semantic_shifts(embeddings, emotions, threshold=0.5):\n",
        "    cosine_similarities = cosine_similarity(embeddings)\n",
        "    segment_boundaries = [0]\n",
        "\n",
        "    for i in range(1, len(embeddings)):\n",
        "        if cosine_similarities[i-1][i] < threshold:\n",
        "            segment_boundaries.append(i)\n",
        "\n",
        "    return segment_boundaries\n",
        "\n",
        "# Segment the Poem\n",
        "def segment_poem(sentences, segment_boundaries):\n",
        "    segments = []\n",
        "    current_segment = []\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        if i in segment_boundaries and current_segment:\n",
        "            segments.append(' '.join(current_segment))\n",
        "            current_segment = []\n",
        "        current_segment.append(sentence)\n",
        "\n",
        "    if current_segment:\n",
        "        segments.append(' '.join(current_segment))\n",
        "\n",
        "    return segments\n",
        "\n",
        "# Main function to run the algorithm\n",
        "def segment_poem_algorithm(poem):\n",
        "    sentences = preprocess(poem)\n",
        "    embeddings, emotions = emotion(sentences)\n",
        "    segment_boundaries = semantic_shifts(embeddings, emotions)\n",
        "    segments = segment_poem(sentences, segment_boundaries)\n",
        "    return segments\n",
        "\n",
        "poem = \"\"\"\n",
        "The rolls and harrows lie at rest beside\n",
        "The battered road and spreading far and wide\n",
        "Above the russet clods the corn is seen\n",
        "Sprouting its spiry points of tender green\n",
        "Where squats the hare to terrors wide awake\n",
        "Like some brown clod the harrows failed to break\n",
        "While neath the warm hedge boys stray far from home\n",
        "To crop the early blossoms as they come\n",
        "Where buttercups will make them eager run\n",
        "Opening their golden caskets to the sun\n",
        "To see who shall be first to pluck the prize\n",
        "And from their hurry up the skylark flies\n",
        "And oer her half formed nest with happy wings\n",
        "Winnows the air – till in the cloud she sings\n",
        "Then hangs a dust spot in the sunny skies\n",
        "And drops and drops till in her nest she lies\n",
        "Where boys unheeding past – neer dreaming then\n",
        "That birds which flew so high would drop again\n",
        "To nests upon the ground where anything\n",
        "May come at to destroy had they the wing\n",
        "Like such a bird, themselves would be too proud\n",
        "And build on nothing but a passing cloud\n",
        "As free from danger as the heavens are free\n",
        "From pain and toil – there would they build and be\n",
        "And sail about the world to scenes unheard\n",
        "Of and unseen – O were they but a bird\n",
        "So think they while they listen to its song\n",
        "And smile and fancy and so pass along\n",
        "While its low nest moist with the dews of morn\n",
        "Lies safely with the leveret in the corn\n",
        "\"\"\"\n",
        "segments = segment_poem_algorithm(poem)\n",
        "for i, segment in enumerate(segments):\n",
        "    print(f\"Segment {i+1}:\\n{segment}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "k2sh38_8hTA_",
        "outputId": "ca3bf627-31d8-4c6a-d801-890a155d93ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fd42e8579513>\u001b[0m in \u001b[0;36m<cell line: 93>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mLies\u001b[0m \u001b[0msafely\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mleveret\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcorn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_poem_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Segment {i+1}:\\n{segment}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-fd42e8579513>\u001b[0m in \u001b[0;36msegment_poem_algorithm\u001b[0;34m(poem)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msegment_poem_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0msegment_boundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msemantic_shifts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_poem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-fd42e8579513>\u001b[0m in \u001b[0;36memotion\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0memotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all-MiniLM-L6-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0memotion_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment-analysis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'j-hartmann/emotion-english-distilroberta-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Batches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0msentences_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    914\u001b[0m                 \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \"\"\"\n\u001b[0;32m--> 916\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sentence_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts, padding)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtext_tuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mbatch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mbatch2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0mto_tokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Ensure required NLTK data packages are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Example text\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a bad  field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate goal of NLP is to enable computers to understand, interpret, and generate human languages in a best way that is valuable. Recent advances in deep learning have led to significant improvements in NLP, making it an exciting time for researchers and practitioners in the field.\n",
        "\"\"\"\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Tokenized Sentences:\")\n",
        "print(sentences)\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Compute sentence embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "print(\"\\nSentence Embeddings:\")\n",
        "print(embeddings)\n",
        "\n",
        "# Compute cosine similarity between sentence embeddings\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "print(\"\\nCosine Similarity Matrix:\")\n",
        "print(similarity_matrix)\n",
        "\n",
        "# Example of using a Hugging Face pipeline for emotion detection\n",
        "emotion_pipeline = pipeline('sentiment-analysis', model='j-hartmann/emotion-english-distilroberta-base')\n",
        "emotion_results = emotion_pipeline(sentences)\n",
        "print(\"\\nEmotion Detection Results:\")\n",
        "for sentence, result in zip(sentences, emotion_results):\n",
        "    print(f\"Sentence: {sentence}\\nEmotion: {result['label']}, Score: {result['score']}\\n\")\n"
      ],
      "metadata": {
        "id": "gU69JSoBhYcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b82443-d6b1-4f4c-f0a5-3ca981c25844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentences:\n",
            "['\\nNatural Language Processing (NLP) is a bad  field of artificial intelligence that focuses on the interaction between computers and humans through natural language.', 'The ultimate goal of NLP is to enable computers to understand, interpret, and generate human languages in a best way that is valuable.', 'Recent advances in deep learning have led to significant improvements in NLP, making it an exciting time for researchers and practitioners in the field.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Embeddings:\n",
            "[[ 0.01029987  0.00301688  0.11539152 ...  0.10410441  0.05939936\n",
            "   0.01007481]\n",
            " [-0.04734892  0.0380333   0.03890233 ...  0.16188876  0.0619978\n",
            "  -0.01987495]\n",
            " [-0.01379908 -0.04659471  0.11711517 ...  0.01230213  0.00945222\n",
            "   0.04243837]]\n",
            "\n",
            "Cosine Similarity Matrix:\n",
            "[[1.        0.7194168 0.6075529]\n",
            " [0.7194168 1.0000001 0.6144576]\n",
            " [0.6075529 0.6144576 0.9999998]]\n",
            "\n",
            "Emotion Detection Results:\n",
            "Sentence: \n",
            "Natural Language Processing (NLP) is a bad  field of artificial intelligence that focuses on the interaction between computers and humans through natural language.\n",
            "Emotion: disgust, Score: 0.8552248477935791\n",
            "\n",
            "Sentence: The ultimate goal of NLP is to enable computers to understand, interpret, and generate human languages in a best way that is valuable.\n",
            "Emotion: neutral, Score: 0.9304348230361938\n",
            "\n",
            "Sentence: Recent advances in deep learning have led to significant improvements in NLP, making it an exciting time for researchers and practitioners in the field.\n",
            "Emotion: neutral, Score: 0.42983129620552063\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "poem = \"\"\"\n",
        "Can someone make my simple wish come true?\n",
        "Male biker seeks female for touring fun.\n",
        "Do you live in North London? Is it you?\n",
        "\n",
        "Gay vegetarian whose friends are few,\n",
        "I'm into music, Shakespeare and the sun.\n",
        "Can someone make my simple wish come true?\n",
        "\n",
        "Executive in search of something new—\n",
        "Perhaps bisexual woman, arty, young.\n",
        "Do you live in North London? Is it you?\n",
        "\n",
        "Successful, straight and solvent? I am too—\n",
        "Attractive Jewish lady with a son.\n",
        "Can someone make my simple wish come true?\n",
        "\n",
        "I'm Libran, inexperienced and blue—\n",
        "Need slim, non-smoker, under twenty-one.\n",
        "Do you live in North London? Is it you?\n",
        "\n",
        "Please write (with photo) to Box 152.\n",
        "Who knows where it may lead once we've begun?\n",
        "Can someone make my simple wish come true?\n",
        "Do you live in North London? Is it you?\n",
        "\"\"\"\n",
        "\n",
        "# Preprocessing the Poem\n",
        "def preprocess(poem):\n",
        "    sentences = [line.strip() for line in poem.split('\\n') if line.strip()]\n",
        "    return sentences\n",
        "\n",
        "def get_sentence_embedding(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "    return embeddings.detach().numpy()\n",
        "\n",
        "def detect_emotion(sentence):\n",
        "    # Detect emotions using the pre-trained model\n",
        "    result = emotion_model(sentence)[0]\n",
        "    # Extract the emotion with the highest score\n",
        "    emotion = max(result, key=lambda x: x['score'])['label']\n",
        "    return emotion\n",
        "\n",
        "def calculate_cosine_similarity(embeddings):\n",
        "    similarities = []\n",
        "    for i in range(len(embeddings) - 1):\n",
        "        sim = cosine_similarity(embeddings[i], embeddings[i+1])\n",
        "        similarities.append(sim[0][0])\n",
        "    return similarities\n",
        "\n",
        "def segment_poem(sentences, boundaries):\n",
        "    segments = []\n",
        "    start = 0\n",
        "    for boundary in boundaries:\n",
        "        segments.append(sentences[start:boundary+1])\n",
        "        start = boundary + 1\n",
        "    segments.append(sentences[start:])\n",
        "    return segments\n",
        "\n",
        "# Main process\n",
        "sentences = preprocess(poem)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "emotion_model = pipeline('text-classification', model='j-hartmann/emotion-english-distilroberta-base', return_all_scores=True)\n",
        "\n",
        "embeddings = [get_sentence_embedding(sentence) for sentence in sentences]\n",
        "emotions = [detect_emotion(sentence) for sentence in sentences]\n",
        "\n",
        "similarities = calculate_cosine_similarity(embeddings)\n",
        "emotion_changes = [emotions[i] != emotions[i+1] for i in range(len(emotions) - 1)]\n",
        "\n",
        "\n",
        "threshold = 0.5\n",
        "boundaries = [i for i, (sim, emo_change) in enumerate(zip(similarities, emotion_changes)) if sim < threshold and emo_change]\n",
        "\n",
        "\n",
        "#\n",
        "\n",
        "# Segment the poem based on identified boundaries\n",
        "segments = segment_poem(sentences, boundaries)\n",
        "\n",
        "# Print segmented poem\n",
        "for i, segment in enumerate(segments):\n",
        "    segment_emotions = emotions[boundaries[i-1]+1 if i > 0 else 0:boundaries[i]+1 if i < len(boundaries) else len(emotions)]\n",
        "    dominant_emotion = max(set(segment_emotions), key=segment_emotions.count)\n",
        "    print(f\"Segment {i+1} (Emotion: {dominant_emotion}):\")\n",
        "    print(' '.join(segment))\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "5oQSxMTIOl6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18d42fd9-a277-4c17-9a17-2487b67aabf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segment 1 (Emotion: neutral):\n",
            "Can someone make my simple wish come true?\n",
            "\n",
            "Segment 2 (Emotion: neutral):\n",
            "Male biker seeks female for touring fun.\n",
            "\n",
            "Segment 3 (Emotion: neutral):\n",
            "Do you live in North London? Is it you?\n",
            "\n",
            "Segment 4 (Emotion: sadness):\n",
            "Gay vegetarian whose friends are few,\n",
            "\n",
            "Segment 5 (Emotion: joy):\n",
            "I'm into music, Shakespeare and the sun.\n",
            "\n",
            "Segment 6 (Emotion: neutral):\n",
            "Can someone make my simple wish come true? Executive in search of something new—\n",
            "\n",
            "Segment 7 (Emotion: disgust):\n",
            "Perhaps bisexual woman, arty, young.\n",
            "\n",
            "Segment 8 (Emotion: neutral):\n",
            "Do you live in North London? Is it you?\n",
            "\n",
            "Segment 9 (Emotion: joy):\n",
            "Successful, straight and solvent? I am too—\n",
            "\n",
            "Segment 10 (Emotion: disgust):\n",
            "Attractive Jewish lady with a son.\n",
            "\n",
            "Segment 11 (Emotion: neutral):\n",
            "Can someone make my simple wish come true?\n",
            "\n",
            "Segment 12 (Emotion: fear):\n",
            "I'm Libran, inexperienced and blue—\n",
            "\n",
            "Segment 13 (Emotion: neutral):\n",
            "Need slim, non-smoker, under twenty-one. Do you live in North London? Is it you?\n",
            "\n",
            "Segment 14 (Emotion: neutral):\n",
            "Please write (with photo) to Box 152.\n",
            "\n",
            "Segment 15 (Emotion: neutral):\n",
            "Who knows where it may lead once we've begun? Can someone make my simple wish come true? Do you live in North London? Is it you?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZhXPOIntHxDn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}